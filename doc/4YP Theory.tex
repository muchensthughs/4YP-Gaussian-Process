\documentclass[11pt]{report}
\title{The Future of Work: \\ Machine Learning and Employment}
\author{Mu Chen}

\usepackage[margin=0.787in]{geometry}

%\usepackage{fontspec}
%\setmainfont{Arial}


\usepackage{setspace}


\usepackage{cite}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{array}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{tablefootnote}
\usepackage[T1]{fontenc}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

\renewcommand{\thefootnote}{\roman{footnote}}

\newcounter{FootnoteCounter} % Can we use the counter functionality for footnotes as it allows footnotes to be inserted into tables, figs etc.
\stepcounter{FootnoteCounter} % Add one to counter (initialised at zero)
%\footnotemark[\arabic{FootnoteCounter}] // Place the footnote number in the text
%\footnotetext[\arabic{FootnoteCounter}]{Example footnote} // Set the text to be displayed with this instance of footnote
%\stepcounter{FootnoteCounter} // Increment the counter for the next useage.

\usepackage{fancyhdr}
 
\usepackage{pdfpages}
 
 
\pagestyle{fancy}

\fancyfoot[C]{\sffamily\fontsize{9pt}{9pt}\selectfont\thepage}

\setlength{\textfloatsep}{\baselineskip + 0.2\baselineskip - 0.1\baselineskip}


\doublespace

\begin{document}
\pagenumbering{roman}
\newpage
\maketitle
 
\rhead{Table of Contents}
\lhead{} 
 
\tableofcontents

\begin{spacing}{2}

\newpage
\pagenumbering{arabic}
\setcounter{page}{1}


\rhead{Introduction}
\section{Introduction}
The question of how computers can change future careers have been a popular issue recently. With the fast development of automation technologies, more jobs have become purely automated. Many people argued that the increase in unemployment rate is partly because this. Would it be true that more people will become jobless? Or is it just a short term phenomenon that does not represent future trends? This report represent how this problem is tackled in the respect of machine learning techniques. In particular, the property of jobs are analysed through Gaussian Process and the possibility of automation of the job can be predicted. 

\newpage
\rhead{Gaussian Process Regression}
\section{Gaussian Process Regression}

\subsection{Covariance Function}
The covariance function describes how observations are related to each other.

\[k(x,x\prime) = \sigma_f^2exp\big[-\frac{(x-x\prime)}{2l^2}\big]\]

\subsection{Hyperparameters}
To optimise the hyperparameters using Newton's method, we need the gradients of log likelihood of $\theta$ wrt. the hyperparameters $\theta_i$.  
 
\begin{equation}
\begin{split}
\frac{\partial}{\partial \theta_i} logp(y|X,\theta) & = \frac{1}{2}y^T K^{-1}\frac{\partial K}{\partial \theta_i}K^{-1}y \, - \, \frac{1}{2}tr(K^{-1}\frac{\partial K}{\partial \theta_i}) \\ & = \frac{1}{2}tr\big( (\alpha\alpha^T - K^{-1})\frac{\partial K}{\partial \theta_i}\big) \quad \mbox{where}\quad\alpha = K^{-1}y
\end{split}
\end{equation}

\rhead{Gaussian Process Classification}
\section{Gaussian Process Classification}
Gaussian Process classification can be simply derived from GP regression except for the fact that outputs are now discrete numbers representing class labels (typically using 1 and -1 for binary class problems). Here, we introduce a latent variable f on which regression will be applied. Then the final step would be 'squashing' f into range [0,1] to give the propability of a certain class.


\subsection{'Squashing' Function}
The 'squashing' function can be any sigmoid[explanation] funciton. Two typical sigmoid functions are logistic function $\lambda (f)$ and cumulative Gaussian function $\phi (f)$.

Inference is hence devided into two steps. First is to compute  the distribution of the prediction latent variable $f_*$ in terms of previous observations 

\[p(\boldsymbol f_*|\boldsymbol X,\boldsymbol y,\boldsymbol x_*) = \int p(\boldsymbol f_*|\boldsymbol X,\boldsymbol x_*,\boldsymbol f)p(\boldsymbol f|\boldsymbol X,\boldsymbol y)\ d\boldsymbol f\]

where \(p(\boldsymbol f|\boldsymbol X,\boldsymbol y) = {p(\boldsymbol y|\boldsymbol f)p(\boldsymbol f|\boldsymbol X)}/{p(\boldsymbol y|\boldsymbol X)}\) is the posterior of latent variables $\boldsymbol f$, which is estimated by MAP with respect to hyperpamaters($l$ and $\sigma_f$).

After obtaining the information about predictive latent variable, the probabilistic prediction is estimated by

\begin{equation}
\bar{\pi}_* = p(y_* = +1|X,y,x_*) = \int \sigma (f_*)p(f_*|X,y,x_*) df_*
\end{equation}


This is called the averaged probility. One may argue that expectation of the prediction should be simply equal to the sigmoidal mean of $f_*$. These are actually two different things because of the non-linearity of sigmoid function, the first is $E[\pi_*|X,y,x_*]$ while the later one is $\sigma (E[f_*|y])$. However, the numerical values of these two are the same for binary classifications. 

\subsection{The Laplace Approximation}

The integral for computing $p(f_*| X, y, x_*)$ is not analytically tractable. By doing a second order Taylor expansion of log$p(f|X,y)$ around its maximum point, we could obtain a Gaussian approximation of the posterior 

\begin{equation}\label{LaplaceApp}
q(f_*|X,y, x_*) = \mathcal{N}(f|\hat{f},A^{-1}) \propto exp(-\frac{1}{2}(f-\hat{f})^T A(f-\hat{f})) \,\sim\, p(f_*|X,y, x_*)
\end{equation}

where $\hat{f} = argmax_f p(f|X,y)$ and $A = -\bigtriangledown\bigtriangledown logp(f|X,y)|_{f=\hat{f}}$

\subsection{Building the Model}


\subsection{Probabilistic Predictions}



\subsection{Mathematical Implementation}
\space

\subsubsection{Optimising latent variable $f$}
The best latent f should be estimated for each set of hyperparameters by solving 

\[\hat{f} = K(\bigtriangledown logp(y|\hat{f}))\]

Commonly used convergence criteria depends on the difference between successive $\Psi (f)$, the magnitude of gradient vetor $\bigtriangledown\Psi (f)$ or the defference between successive values of $f$. Note in practice, the convergence of objective function is assured by checking that each iteration gives an increase in $\Psi (f)$. If not, a smaller step change in $f$ should be used.


\subsubsection{Maximising Marginal Likelihood}

Again, as in regression, we need to find the gradient of likelihood wrt. all hyperparameters we are trying to optimise. The case is slightly more difficult than regression as we introduced a latent variable $f$ and used Laplace's Approximation for likelihood itself. 

Since kovariance matrix $K$ is a function of hyperparameters, therefore $\hat{f}$ and $W$ are implicitly functions of hyperparameters. The derivative of equation\ref{LaplaceApp} would be


\begin{equation}
\frac{\partial logq(y|X,\theta)}{\partial \theta_i} = \frac{\partial logq(y|X,\theta)}{\partial \theta_i}\bigg|_{explicit} + \sum_{i=1}^n \frac{\partial logq(y|X,\theta)}{\partial \hat{f}_i}\frac{\partial \hat{f}_i}{\partial \theta_i}
\end{equation}

where


\begin{align} 
\label{eq:al1}
\frac{\partial logq(y|X,\theta)}{\partial \theta_i}\bigg|_{explicit} &= \frac{1}{2}\hat{f}^T K^{-1}\frac{\partial K}{\partial \theta_i}K^{-1}\hat{f} - \frac{1}{2}tr\Big((W^{-1}+K)^{-1}\frac{\partial K}{\partial \theta_i}\Big) \\ 
\label{eq:al2}
\frac{\partial logq(y|X,\theta)}{\partial \hat{f}_i} &= -\frac{1}{2} \big[(K^{-1}+W)^{-1}\big]_{ii}\frac{\partial^3}{\partial f_i^3}logp(y|\hat{f})
\end{align}



\subsubsection{Predictive Class Probability}
Adding jitter - Jitters are added in the diagonal of covariance matrix to make sure that K is positive definite. 
\subsection{Multi-class Classification}


\end{spacing}
\end{document}